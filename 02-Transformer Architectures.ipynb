{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b13884-ed4e-4392-8db5-05c0cbcafa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9938f-01bf-43b4-b8fa-d498e0198cfa",
   "metadata": {},
   "source": [
    "## BART - The Encoder/Decoder Architecture\n",
    "Encoder/Decoders strive to reconstitute the original input. They're useful for things like:\n",
    "- Translations (e.g. French to English)\n",
    "- Generating captions from images\n",
    "- Summarization tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd2f4b3-3907-403e-86bf-621bdf6c1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "BART = AutoModel.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff5f50-eb28-435c-9a27-cb72c3d53b8e",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "- Bart consists of 12 encoder layers and 12 decoder layers\n",
    "- The encoder component utilizes self attention.\n",
    "- Additionally, the decoder layer features `encoder_attn` which is an implementation known as **cross-attention**.\n",
    "- Cross attention can be thought of as a common link between the encoder and decoder components. Because of it, the decoder knows which part of the encoder's output are the most relevant parts of its information. The cross-attention component will condition the decoder output based on the encoder representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac301d6c-bf8b-4948-8984-5af303fcd1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartModel(\n",
      "  (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "  (encoder): BartEncoder(\n",
      "    (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartEncoderLayer(\n",
      "        (self_attn): BartSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): BartDecoder(\n",
      "    (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartDecoderLayer(\n",
      "        (self_attn): BartSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print( BART )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c830f-7132-4b8b-b1b6-2cbf8ea0654b",
   "metadata": {},
   "source": [
    "### Summarization with BART\n",
    "- This uses the `pipeline` feature of the `transformers` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ae87f5-5828-4127-a427-e82cec28ba18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ea32140e244989973e8c8913b980ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8880fb5a604884840dcf18650d6b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ea5c1370f84e0a82c3fac01d19da1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1c012af30c4144a526bdef7e26b676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a553817eac5142c69d5484220d0afe04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808452d40ff045d294a703864769bb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bennett and Gaga became fast friends and close collaborators. They recorded two albums together, 2014's \"Cheek to Cheek\" and 2021's \"Love for Sale\"\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "sum = summarizer(\"\"\"Gaga was best known in the 2010s for pop hits like “Poker Face” and avant-garde experimentation on albums like “Artpop,” and Bennett, a singer who mostly stuck to standards, was in his 80s when the pair met. And yet Bennett and Gaga became fast friends and close collaborators, which they remained until Bennett’s death at 96 on Friday. They recorded two albums together, 2014’s “Cheek to Cheek” and 2021’s “Love for Sale,” which both won Grammys for best traditional pop vocal album.\"\"\", min_length=20, max_length=50)\n",
    "print( sum[0]['summary_text'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24c303-d8f9-4399-b06a-41bc525aa389",
   "metadata": {},
   "source": [
    "## BERT - The Encoder-Only Architecture\n",
    "\n",
    "The encoder-only models are created by stacking multiple encoder components. Because the encoder output is not coupled to a decoder, it can only be used to find a vector encoding for the input. It can also be paired with a classification head (feedforward layer, on top) to help with label prediction.\n",
    "\n",
    "A fundamental distinction in the encoder-only architecture is the absence of a masked self-attention l ayer used in decoder models to prevent future tokens from influencing the current token during training. In encoder-only models, the self-attention mechanism processes the entire input sequence simultaneously, allwing the model to capture the full context. This makes them exceptionally well suited for generating comprehensive vector representations of documents, ensuring that all information is retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f029e302-dc29-419b-8e09-7c0e532cc3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789dd3a087b349e296f14b3f70bf6bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf207539a0794bbeb52ae20c1719eb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BERT = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f912cc85-7412-4e53-95e1-6f7626c0a46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print( BERT )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712f46f-70ef-4034-9649-54db7a33b234",
   "metadata": {},
   "source": [
    "### More about BERT\n",
    "BERT introduced the encoder-only model. It improved state-of-the-art scores on various NLP tasks. The model itself is pre-trained with two learning objectives:\n",
    "1. **Masked Language Modeling (MLM)** - Random tokens in the input are masked, and the model is trained to predict these masked tokens, allowing it to learn deep bidirectional representations.\n",
    "2. **Next Sentence Prediction (NSP)** - Sentences are presented in pairs, and the model is trained to determine whether the first sentence entails the second, helping it understand sentence relationships.\n",
    "\n",
    "BERT follows the traditional transformer architecture with 12 stacked encoder blocks. However, the network's output will be passed on to a **pooler layer**, a **feedforward linear layer**, **followed by non-linearity** that will construct the final representation. This representation will be used for other tasks like **classification and similarity assessment**.\n",
    "\n",
    "## Sentiment Analysis\n",
    "This next cell utilizes a fine-tuned version of the BERT model for sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e3a3d4-457f-4f79-8959-c2817d55f903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fded856305428d94cb62a5cfd8390b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7e2c1001ab4b088bfa98d05557bdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a724507517046fb92d721f5d4565d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ab84df55134fe1816e7bc638d6f78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46d96ce8a5f45b5843060d26e42890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '5 stars', 'score': 0.8550481200218201}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "lbl = classifier(\"\"\"This restaurant is awesome.\"\"\")\n",
    "\n",
    "print( lbl )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de3ebf-ccaf-46e5-9a83-4d8137aa2450",
   "metadata": {},
   "source": [
    "## GPT-2 - The Decoder-Only Architecture\n",
    "These models primarily focus on predicting the next token of their output. Scaling up the decoder-only models can considerably imprive the network's language understanding and generalization capabilities. As a result, a single model can excel at various tasks just by employing varied prompts, such as:\n",
    "- Classification\n",
    "- Summarization\n",
    "- Translation\n",
    "  \n",
    "Just as well as an encoder-only model.\n",
    "\n",
    "LLMs such as those in the GPT family, are pre-trained with the Causal Language Modeling objective. This means the model attempts to predict the next word, whereas the attention mechanism can only attend to previous tokens on the left. This in turn means the model can only anticipate the next token based on the previous context and cannot peek at future tokens, avoiding cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0db811-fdb4-47ab-b3eb-4ce249ebd6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40f8156580b40aa91f20d77b9f526f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43332ea7098e45f4a88772a440a8436b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = AutoModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743576a1-5e29-455e-9865-805f5c30515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print( gpt2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6857cc6-d220-4421-9dc4-8923598b62be",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020dddc9-afdd-4bd9-b22e-022c18e464f4",
   "metadata": {},
   "source": [
    "As you can see from the output above, this is the typical transformer decoder block **without the cross-attention layer**.\n",
    "\n",
    "The GPT family also uses distinct linear layers (Conv1D) to transpose the weights. This is not related to PyTorch's convolutional layer. This design choice is unique to OpenAI, whereas other large open-source language models employ the conventional linear layer.\n",
    "\n",
    "The following example uses GPT-2 for text prediction. It generates four possibilities to complete the statement: \"This movie was a very\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067bbba2-3979-48b4-90df-190b002848c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e364e9a6b194b9eabf76094cb09c893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd8502192dc4866bdb530ca5e9caf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871ac5e59b294e9fa6a5a93a79c08725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ca8d59ff044216a381c79900407bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98d8aaeff20406786550a2eb8b7798b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00910d3-e6d7-4543-ab0f-d3f8783e33ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = generator(\"This movie was a very\", do_sample=True, top_p=0.95, num_return_sequences=4, max_new_tokens=50, return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d09363-50c3-451d-9377-4b3574d2e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  emotional moment for me. My wife was so supportive and so kind. It brought back something that didn't happen in my previous films… A lot of emotions, from sadness to happiness… She was a perfect person for this story. She knew how to\n",
      ">  good one to watch and I had one great time.\n",
      "\n",
      "\n",
      "\"I love the art of the craftsmanship,\" said producer Jeff Greening. \"I just think it's funny. I like to put all these drawings together.\"\n",
      "\n",
      "Greening\n",
      ">  nice project that made me want to see more of it than any other. A very nice, fun show, for people who love to watch movies. I love The Hunger Games but because of how much it inspired me to make this and so much more\n",
      ">  successful and extremely rewarding experience for me, as I got to go inside the film and learn so much. And the film set the tone for me, as it set the tone for people that know me. That's really what's going on, and\n"
     ]
    }
   ],
   "source": [
    "for item in output:\n",
    "  print( \">\", item['generated_text'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfb0d6-0795-4b75-a67e-527a9ea5ce03",
   "metadata": {},
   "source": [
    "## Transformer Architecture Optimization Techniques\n",
    "\n",
    "The more input a transformer model receives, the more context it has to generate a text continuation appropriately. For this reason, input is commonly referred to, generally, as *\"context\"*.\n",
    "\n",
    "As the context length expands, the computational resouces required for training and inference increase substantially. **See: [Quadratic time and space complexity](https://github.com/bradtraversy/traversy-js-challenges/blob/main/05-complexity/05-quadratic-time-complexity/readme.md)**\n",
    "\n",
    "This complexity scales from two primary operations:\n",
    "1. The *multiplication* between the embeddings of the input tokens with the *learned matrices* (which creates the Query, Key and Value matrices—which entails O(nd**2) where *n* is the number of input tokens)\n",
    "2. The *multiplication* of the Query, Key and Value matrices.\n",
    "\n",
    "In regards to the *attention layer*, the computational complexity grows **quadratically** as the context length *or* the number of embeddings increases.\n",
    "\n",
    "Optimization is clearly necessary when models are used as an assistant with documents or corpi of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b0094c-22ac-47f6-adda-9bfdf21df16d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
