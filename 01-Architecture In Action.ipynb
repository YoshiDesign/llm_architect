{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f053245-fece-4a56-92d4-96cdc4396522",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- Tiktokenizer\n",
    "- Excalidraw\n",
    "- Andrej Karpathy (Youtube) https://www.youtube.com/watch?v=7xTGNNLPyMI\n",
    "- [Quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2a65d-f3e7-4774-a8ea-94319e849492",
   "metadata": {},
   "source": [
    "### My Notes\n",
    "- In this Notebook we're using Facebook's Open Pretrained Transformer (OPT)\n",
    "- The `tokenizer` object loads the vocabulary required to interact with the model and converts the sample input (`inp` variable) to token IDs and attention mask\n",
    "- The **attention mask** is a vector designed to help ignore specific tokens. (All 1's in this example, meaning we're not omitting anything)\n",
    "- We use `print(OPT_model.model)` to inspect the model's architecture. You'll notice `OPTDecoder` in the output, referring to the **Decoder** Only model\n",
    "- `return_tensors=\"pt\"` in our tokenizer() instantiation means we want our tokens returned as PyTorch Tensors, not NumPy arrays (default)\n",
    "- Use `nvidia-smi` to check your VRAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b0b8d0-359e-4ac1-8d70-f856be89fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.11/site-packages (4.50.0)\n",
      "Requirement already satisfied: accelerate in ./venv/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.11/site-packages (0.45.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./venv/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./venv/lib/python3.11/site-packages (from accelerate) (2.6.0+cu118)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b7599c6-5c13-456f-b9ef-0f8c21e78e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "#import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_name = \"facebook/opt-1.3b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define quantization config -- This enables efficient utilization on consumer GPUs\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677c6cd-66a2-41d3-9949-9ba3ee3d4b59",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894ed3e-3d5e-4164-956b-3b3afca0a6ff",
   "metadata": {},
   "source": [
    "Note that the `.to()` method is applied to both the Model and the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4b5ea9-9200-4f59-92d6-d9e6c23051fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "OPT_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, low_cpu_mem_usage=True)\n",
    "OPT_model.to(device) # Model is on the GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c22a0abc-ae26-43ee-8748-7eac445491f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "{'input_ids': tensor([[    2,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "inp = \"The quick brown fox jumps over the lazy dog\"\n",
    "inp_tokenized = tokenizer( inp, return_tensors=\"pt\" ).to(device) # Tokenizer is on the GPU. \"pt\" for PyTorch tensor format\n",
    "\n",
    "print( inp_tokenized['input_ids'].size() )\n",
    "print( inp_tokenized )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e605d-040e-4734-9d09-b91efc539b34",
   "metadata": {},
   "source": [
    "## Analyzing the Model\n",
    "- This is a `decoder`-only model, a common choice for transformer-based language models. The `decoder` key grants us access to its inner workings.\n",
    "- the `layers` key reveals that the decoder component comprises 24 stacked layers with the same design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "250aff6f-3db7-4bd0-9db4-652ea880ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTModel(\n",
      "  (decoder): OPTDecoder(\n",
      "    (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x OPTDecoderLayer(\n",
      "        (self_attn): OPTSdpaAttention(\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "          (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
      "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print( OPT_model.model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2cc05a-9dc6-4094-b3ca-8d75504cff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t Embedding(50272, 2048, padding_idx=1)\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-0.0407,  0.0519,  0.0574,  ..., -0.0263, -0.0355, -0.0260],\n",
      "         [-0.0371,  0.0220, -0.0096,  ...,  0.0265, -0.0166, -0.0030],\n",
      "         [-0.0455, -0.0236, -0.0121,  ...,  0.0043, -0.0166,  0.0193],\n",
      "         ...,\n",
      "         [ 0.0007,  0.0267,  0.0257,  ...,  0.0622,  0.0421,  0.0279],\n",
      "         [-0.0126,  0.0347, -0.0352,  ..., -0.0393, -0.0396, -0.0102],\n",
      "         [-0.0115,  0.0319,  0.0274,  ..., -0.0472, -0.0059,  0.0341]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedded_input = OPT_model.model.decoder.embed_tokens(inp_tokenized['input_ids'])\n",
    "print( \"Layer:\\t\", OPT_model.model.decoder.embed_tokens )\n",
    "print( \"Size:\\t\", embedded_input.size() )\n",
    "print( \"Output:\\t\", embedded_input )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048602d-6ab2-45c2-814f-acbf89c4664e",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The embedding layer is accessed via the decoder object's `.embed_tokens` method, which we use to deliver our tokenized inputs to the layer.\n",
    "\n",
    "In the next cell, we call upon `embed_positions` which delivers our positional signal to each layer of the decoder.\n",
    "This lets our model access positional information, which allows it to consider the order of words in the sequence more effectively."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e963098-c508-4a48-9223-44220828b490",
   "metadata": {},
   "source": [
    "embed_pos_input = OPT_model.model.decoder.embed_positions(inp_tokenized['attention_mask'])\n",
    "print( \"Layer:\\t\", OPT_model.model.decoder.embed_positions )\n",
    "print( \"Size:\\t\", embed_pos_input.size() )\n",
    "print( \"Output:\\t\", embed_pos_input )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812a5de-e966-449d-a248-2f9893076ef3",
   "metadata": {},
   "source": [
    "#### Finally...\n",
    "...for the sake of some output, we'll access the first layers self-attention component by indexing through the layers and using the `self_attn` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "571c6e34-fbca-4e2d-9d67-17d23a8446c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t OPTSdpaAttention(\n",
      "  (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "  (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "  (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "  (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      ")\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-0.0139, -0.0067,  0.0023,  ...,  0.0073, -0.0010,  0.0130],\n",
      "         [-0.0132, -0.0082,  0.0026,  ...,  0.0089,  0.0010,  0.0123],\n",
      "         [-0.0134, -0.0048,  0.0041,  ...,  0.0101,  0.0029,  0.0140],\n",
      "         ...,\n",
      "         [-0.0124, -0.0094,  0.0054,  ...,  0.0097,  0.0022,  0.0102],\n",
      "         [-0.0122, -0.0098,  0.0055,  ...,  0.0097,  0.0018,  0.0096],\n",
      "         [-0.0121, -0.0107,  0.0059,  ...,  0.0097,  0.0021,  0.0096]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoshi/pr0/llm/venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_position_input = embedded_input + embed_pos_input\n",
    "hidden_states, _, _ = OPT_model.model.decoder.layers[0].self_attn( embed_position_input )\n",
    "print( \"Layer:\\t\", OPT_model.model.decoder.layers[0].self_attn )\n",
    "print( \"Size:\\t\", hidden_states.size() )\n",
    "print( \"Output:\\t\", hidden_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338a097-0562-4aad-9e9b-40e90ba68bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
